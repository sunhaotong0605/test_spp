{
  "best_metric": null,
  "best_model_checkpoint": null,
  "epoch": 300.0,
  "eval_steps": 500,
  "global_step": 195300,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.7680491551459293,
      "grad_norm": 0.11093078553676605,
      "learning_rate": 9.969254419677172e-05,
      "loss": 0.4877,
      "step": 500
    },
    {
      "epoch": 1.5360983102918588,
      "grad_norm": 0.07883632928133011,
      "learning_rate": 9.930822444273636e-05,
      "loss": 0.3593,
      "step": 1000
    },
    {
      "epoch": 2.3041474654377883,
      "grad_norm": 0.05547349154949188,
      "learning_rate": 9.8923904688701e-05,
      "loss": 0.3495,
      "step": 1500
    },
    {
      "epoch": 3.0721966205837172,
      "grad_norm": 0.05972512438893318,
      "learning_rate": 9.853958493466566e-05,
      "loss": 0.3451,
      "step": 2000
    },
    {
      "epoch": 3.8402457757296466,
      "grad_norm": 0.12621435523033142,
      "learning_rate": 9.815526518063029e-05,
      "loss": 0.3422,
      "step": 2500
    },
    {
      "epoch": 4.6082949308755765,
      "grad_norm": 0.01357550173997879,
      "learning_rate": 9.777094542659493e-05,
      "loss": 0.3388,
      "step": 3000
    },
    {
      "epoch": 5.376344086021505,
      "grad_norm": 0.24463807046413422,
      "learning_rate": 9.738662567255957e-05,
      "loss": 0.3363,
      "step": 3500
    },
    {
      "epoch": 6.1443932411674345,
      "grad_norm": 0.37808123230934143,
      "learning_rate": 9.700230591852421e-05,
      "loss": 0.3322,
      "step": 4000
    },
    {
      "epoch": 6.912442396313364,
      "grad_norm": 0.2644215524196625,
      "learning_rate": 9.661798616448886e-05,
      "loss": 0.3278,
      "step": 4500
    },
    {
      "epoch": 7.680491551459293,
      "grad_norm": 0.18943887948989868,
      "learning_rate": 9.623366641045351e-05,
      "loss": 0.3242,
      "step": 5000
    },
    {
      "epoch": 8.448540706605222,
      "grad_norm": 0.13406307995319366,
      "learning_rate": 9.584934665641815e-05,
      "loss": 0.3206,
      "step": 5500
    },
    {
      "epoch": 9.216589861751151,
      "grad_norm": 0.08183730393648148,
      "learning_rate": 9.54650269023828e-05,
      "loss": 0.3147,
      "step": 6000
    },
    {
      "epoch": 9.98463901689708,
      "grad_norm": 0.029629390686750412,
      "learning_rate": 9.508070714834742e-05,
      "loss": 0.3093,
      "step": 6500
    },
    {
      "epoch": 10.75268817204301,
      "grad_norm": 0.15152116119861603,
      "learning_rate": 9.469638739431207e-05,
      "loss": 0.3034,
      "step": 7000
    },
    {
      "epoch": 11.52073732718894,
      "grad_norm": 0.34367215633392334,
      "learning_rate": 9.431206764027671e-05,
      "loss": 0.2991,
      "step": 7500
    },
    {
      "epoch": 12.288786482334869,
      "grad_norm": 0.4453345835208893,
      "learning_rate": 9.392774788624135e-05,
      "loss": 0.2953,
      "step": 8000
    },
    {
      "epoch": 13.056835637480798,
      "grad_norm": 0.2846039831638336,
      "learning_rate": 9.3543428132206e-05,
      "loss": 0.2917,
      "step": 8500
    },
    {
      "epoch": 13.824884792626728,
      "grad_norm": 0.05279155448079109,
      "learning_rate": 9.315910837817065e-05,
      "loss": 0.2886,
      "step": 9000
    },
    {
      "epoch": 14.592933947772657,
      "grad_norm": 0.13052129745483398,
      "learning_rate": 9.277478862413529e-05,
      "loss": 0.2855,
      "step": 9500
    },
    {
      "epoch": 15.360983102918587,
      "grad_norm": 0.27343514561653137,
      "learning_rate": 9.239046887009992e-05,
      "loss": 0.2824,
      "step": 10000
    },
    {
      "epoch": 16.129032258064516,
      "grad_norm": 0.29044225811958313,
      "learning_rate": 9.200614911606456e-05,
      "loss": 0.2801,
      "step": 10500
    },
    {
      "epoch": 16.897081413210444,
      "grad_norm": 0.28912124037742615,
      "learning_rate": 9.16218293620292e-05,
      "loss": 0.2768,
      "step": 11000
    },
    {
      "epoch": 17.665130568356375,
      "grad_norm": 0.11962385475635529,
      "learning_rate": 9.123750960799386e-05,
      "loss": 0.2747,
      "step": 11500
    },
    {
      "epoch": 18.433179723502302,
      "grad_norm": 0.23042646050453186,
      "learning_rate": 9.08531898539585e-05,
      "loss": 0.2723,
      "step": 12000
    },
    {
      "epoch": 19.201228878648234,
      "grad_norm": 0.22191612422466278,
      "learning_rate": 9.046887009992314e-05,
      "loss": 0.2702,
      "step": 12500
    },
    {
      "epoch": 19.96927803379416,
      "grad_norm": 0.38959744572639465,
      "learning_rate": 9.008455034588779e-05,
      "loss": 0.2676,
      "step": 13000
    },
    {
      "epoch": 20.737327188940093,
      "grad_norm": 0.20197346806526184,
      "learning_rate": 8.970023059185243e-05,
      "loss": 0.2661,
      "step": 13500
    },
    {
      "epoch": 21.50537634408602,
      "grad_norm": 0.35425201058387756,
      "learning_rate": 8.931591083781706e-05,
      "loss": 0.2633,
      "step": 14000
    },
    {
      "epoch": 22.27342549923195,
      "grad_norm": 0.17495931684970856,
      "learning_rate": 8.893159108378171e-05,
      "loss": 0.2618,
      "step": 14500
    },
    {
      "epoch": 23.04147465437788,
      "grad_norm": 0.22586628794670105,
      "learning_rate": 8.854727132974635e-05,
      "loss": 0.2597,
      "step": 15000
    },
    {
      "epoch": 23.80952380952381,
      "grad_norm": 0.36644306778907776,
      "learning_rate": 8.8162951575711e-05,
      "loss": 0.2581,
      "step": 15500
    },
    {
      "epoch": 24.577572964669738,
      "grad_norm": 0.32001885771751404,
      "learning_rate": 8.777863182167564e-05,
      "loss": 0.2567,
      "step": 16000
    },
    {
      "epoch": 25.34562211981567,
      "grad_norm": 0.13631655275821686,
      "learning_rate": 8.739431206764028e-05,
      "loss": 0.2549,
      "step": 16500
    },
    {
      "epoch": 26.113671274961597,
      "grad_norm": 0.24469105899333954,
      "learning_rate": 8.700999231360492e-05,
      "loss": 0.2534,
      "step": 17000
    },
    {
      "epoch": 26.881720430107528,
      "grad_norm": 0.1953907012939453,
      "learning_rate": 8.662567255956957e-05,
      "loss": 0.2517,
      "step": 17500
    },
    {
      "epoch": 27.649769585253456,
      "grad_norm": 0.322001576423645,
      "learning_rate": 8.624135280553421e-05,
      "loss": 0.2507,
      "step": 18000
    },
    {
      "epoch": 28.417818740399387,
      "grad_norm": 0.2803330719470978,
      "learning_rate": 8.585703305149885e-05,
      "loss": 0.249,
      "step": 18500
    },
    {
      "epoch": 29.185867895545314,
      "grad_norm": 0.2185753434896469,
      "learning_rate": 8.547271329746349e-05,
      "loss": 0.2475,
      "step": 19000
    },
    {
      "epoch": 29.953917050691246,
      "grad_norm": 0.22832916676998138,
      "learning_rate": 8.508839354342813e-05,
      "loss": 0.2465,
      "step": 19500
    },
    {
      "epoch": 30.721966205837173,
      "grad_norm": 0.2636726200580597,
      "learning_rate": 8.470407378939278e-05,
      "loss": 0.2452,
      "step": 20000
    },
    {
      "epoch": 31.490015360983104,
      "grad_norm": 0.16248461604118347,
      "learning_rate": 8.431975403535742e-05,
      "loss": 0.2437,
      "step": 20500
    },
    {
      "epoch": 32.25806451612903,
      "grad_norm": 0.23070091009140015,
      "learning_rate": 8.393543428132207e-05,
      "loss": 0.2426,
      "step": 21000
    },
    {
      "epoch": 33.02611367127496,
      "grad_norm": 0.35079824924468994,
      "learning_rate": 8.355111452728672e-05,
      "loss": 0.241,
      "step": 21500
    },
    {
      "epoch": 33.794162826420894,
      "grad_norm": 0.5390868186950684,
      "learning_rate": 8.316679477325134e-05,
      "loss": 0.2396,
      "step": 22000
    },
    {
      "epoch": 34.56221198156682,
      "grad_norm": 0.269741952419281,
      "learning_rate": 8.278247501921599e-05,
      "loss": 0.2387,
      "step": 22500
    },
    {
      "epoch": 35.33026113671275,
      "grad_norm": 0.46029242873191833,
      "learning_rate": 8.239815526518063e-05,
      "loss": 0.2373,
      "step": 23000
    },
    {
      "epoch": 36.09831029185868,
      "grad_norm": 0.4741738736629486,
      "learning_rate": 8.201383551114527e-05,
      "loss": 0.2361,
      "step": 23500
    },
    {
      "epoch": 36.866359447004605,
      "grad_norm": 0.33099618554115295,
      "learning_rate": 8.162951575710993e-05,
      "loss": 0.2346,
      "step": 24000
    },
    {
      "epoch": 37.634408602150536,
      "grad_norm": 0.27414920926094055,
      "learning_rate": 8.124519600307457e-05,
      "loss": 0.2337,
      "step": 24500
    },
    {
      "epoch": 38.40245775729647,
      "grad_norm": 0.33741554617881775,
      "learning_rate": 8.086087624903921e-05,
      "loss": 0.2323,
      "step": 25000
    },
    {
      "epoch": 39.1705069124424,
      "grad_norm": 0.3054468035697937,
      "learning_rate": 8.047655649500385e-05,
      "loss": 0.231,
      "step": 25500
    },
    {
      "epoch": 39.93855606758832,
      "grad_norm": 0.22497910261154175,
      "learning_rate": 8.009223674096848e-05,
      "loss": 0.2301,
      "step": 26000
    },
    {
      "epoch": 40.706605222734254,
      "grad_norm": 0.24093027412891388,
      "learning_rate": 7.970791698693312e-05,
      "loss": 0.2288,
      "step": 26500
    },
    {
      "epoch": 41.474654377880185,
      "grad_norm": 0.4656359553337097,
      "learning_rate": 7.932359723289778e-05,
      "loss": 0.228,
      "step": 27000
    },
    {
      "epoch": 42.242703533026116,
      "grad_norm": 0.15139907598495483,
      "learning_rate": 7.893927747886242e-05,
      "loss": 0.2268,
      "step": 27500
    },
    {
      "epoch": 43.01075268817204,
      "grad_norm": 0.25540730357170105,
      "learning_rate": 7.855495772482706e-05,
      "loss": 0.2259,
      "step": 28000
    },
    {
      "epoch": 43.77880184331797,
      "grad_norm": 0.21152403950691223,
      "learning_rate": 7.81706379707917e-05,
      "loss": 0.225,
      "step": 28500
    },
    {
      "epoch": 44.5468509984639,
      "grad_norm": 0.2258550226688385,
      "learning_rate": 7.778631821675635e-05,
      "loss": 0.2239,
      "step": 29000
    },
    {
      "epoch": 45.314900153609834,
      "grad_norm": 0.32878267765045166,
      "learning_rate": 7.740199846272099e-05,
      "loss": 0.2234,
      "step": 29500
    },
    {
      "epoch": 46.08294930875576,
      "grad_norm": 0.3345595598220825,
      "learning_rate": 7.701767870868562e-05,
      "loss": 0.2225,
      "step": 30000
    },
    {
      "epoch": 46.85099846390169,
      "grad_norm": 0.33373942971229553,
      "learning_rate": 7.663335895465027e-05,
      "loss": 0.2216,
      "step": 30500
    },
    {
      "epoch": 47.61904761904762,
      "grad_norm": 0.4357468783855438,
      "learning_rate": 7.624903920061492e-05,
      "loss": 0.2207,
      "step": 31000
    },
    {
      "epoch": 48.38709677419355,
      "grad_norm": 0.5250424146652222,
      "learning_rate": 7.586471944657956e-05,
      "loss": 0.2201,
      "step": 31500
    },
    {
      "epoch": 49.155145929339476,
      "grad_norm": 0.19257602095603943,
      "learning_rate": 7.54803996925442e-05,
      "loss": 0.2196,
      "step": 32000
    },
    {
      "epoch": 49.92319508448541,
      "grad_norm": 0.30937570333480835,
      "learning_rate": 7.509607993850884e-05,
      "loss": 0.2188,
      "step": 32500
    },
    {
      "epoch": 50.69124423963134,
      "grad_norm": 0.3694649636745453,
      "learning_rate": 7.471176018447349e-05,
      "loss": 0.218,
      "step": 33000
    },
    {
      "epoch": 51.45929339477726,
      "grad_norm": 0.6759611964225769,
      "learning_rate": 7.432744043043813e-05,
      "loss": 0.2174,
      "step": 33500
    },
    {
      "epoch": 52.22734254992319,
      "grad_norm": 0.9158234596252441,
      "learning_rate": 7.394312067640277e-05,
      "loss": 0.2169,
      "step": 34000
    },
    {
      "epoch": 52.995391705069125,
      "grad_norm": 0.3092997372150421,
      "learning_rate": 7.355880092236741e-05,
      "loss": 0.2162,
      "step": 34500
    },
    {
      "epoch": 53.763440860215056,
      "grad_norm": 0.24045228958129883,
      "learning_rate": 7.317448116833205e-05,
      "loss": 0.2157,
      "step": 35000
    },
    {
      "epoch": 54.53149001536098,
      "grad_norm": 0.2171051800251007,
      "learning_rate": 7.27901614142967e-05,
      "loss": 0.2154,
      "step": 35500
    },
    {
      "epoch": 55.29953917050691,
      "grad_norm": 0.3217175602912903,
      "learning_rate": 7.240584166026134e-05,
      "loss": 0.2145,
      "step": 36000
    },
    {
      "epoch": 56.06758832565284,
      "grad_norm": 0.8940589427947998,
      "learning_rate": 7.202152190622598e-05,
      "loss": 0.2139,
      "step": 36500
    },
    {
      "epoch": 56.83563748079877,
      "grad_norm": 0.2835509181022644,
      "learning_rate": 7.163720215219064e-05,
      "loss": 0.2135,
      "step": 37000
    },
    {
      "epoch": 57.6036866359447,
      "grad_norm": 0.45930641889572144,
      "learning_rate": 7.125288239815528e-05,
      "loss": 0.2132,
      "step": 37500
    },
    {
      "epoch": 58.37173579109063,
      "grad_norm": 0.5313940644264221,
      "learning_rate": 7.08685626441199e-05,
      "loss": 0.2123,
      "step": 38000
    },
    {
      "epoch": 59.13978494623656,
      "grad_norm": 0.5999705791473389,
      "learning_rate": 7.048424289008455e-05,
      "loss": 0.2123,
      "step": 38500
    },
    {
      "epoch": 59.90783410138249,
      "grad_norm": 0.4226417541503906,
      "learning_rate": 7.009992313604919e-05,
      "loss": 0.2113,
      "step": 39000
    },
    {
      "epoch": 60.675883256528415,
      "grad_norm": 0.6715281009674072,
      "learning_rate": 6.971560338201383e-05,
      "loss": 0.2112,
      "step": 39500
    },
    {
      "epoch": 61.443932411674346,
      "grad_norm": 0.37139028310775757,
      "learning_rate": 6.933128362797849e-05,
      "loss": 0.2107,
      "step": 40000
    },
    {
      "epoch": 62.21198156682028,
      "grad_norm": 0.3209153413772583,
      "learning_rate": 6.894696387394313e-05,
      "loss": 0.2103,
      "step": 40500
    },
    {
      "epoch": 62.98003072196621,
      "grad_norm": 0.35185500979423523,
      "learning_rate": 6.856264411990777e-05,
      "loss": 0.2097,
      "step": 41000
    },
    {
      "epoch": 63.74807987711213,
      "grad_norm": 0.2414606809616089,
      "learning_rate": 6.81783243658724e-05,
      "loss": 0.2095,
      "step": 41500
    },
    {
      "epoch": 64.51612903225806,
      "grad_norm": 0.6121761202812195,
      "learning_rate": 6.779400461183704e-05,
      "loss": 0.2088,
      "step": 42000
    },
    {
      "epoch": 65.284178187404,
      "grad_norm": 0.4111347794532776,
      "learning_rate": 6.740968485780169e-05,
      "loss": 0.2088,
      "step": 42500
    },
    {
      "epoch": 66.05222734254993,
      "grad_norm": 0.34480515122413635,
      "learning_rate": 6.702536510376634e-05,
      "loss": 0.2082,
      "step": 43000
    },
    {
      "epoch": 66.82027649769586,
      "grad_norm": 0.4205295443534851,
      "learning_rate": 6.664104534973098e-05,
      "loss": 0.208,
      "step": 43500
    },
    {
      "epoch": 67.58832565284179,
      "grad_norm": 0.5051116347312927,
      "learning_rate": 6.625672559569563e-05,
      "loss": 0.2074,
      "step": 44000
    },
    {
      "epoch": 68.3563748079877,
      "grad_norm": 0.34591296315193176,
      "learning_rate": 6.587240584166027e-05,
      "loss": 0.207,
      "step": 44500
    },
    {
      "epoch": 69.12442396313364,
      "grad_norm": 0.3189299702644348,
      "learning_rate": 6.548808608762491e-05,
      "loss": 0.207,
      "step": 45000
    },
    {
      "epoch": 69.89247311827957,
      "grad_norm": 0.31949853897094727,
      "learning_rate": 6.510376633358954e-05,
      "loss": 0.2064,
      "step": 45500
    },
    {
      "epoch": 70.6605222734255,
      "grad_norm": 0.4847329258918762,
      "learning_rate": 6.47194465795542e-05,
      "loss": 0.2065,
      "step": 46000
    },
    {
      "epoch": 71.42857142857143,
      "grad_norm": 0.5389507412910461,
      "learning_rate": 6.433512682551884e-05,
      "loss": 0.2052,
      "step": 46500
    },
    {
      "epoch": 72.19662058371736,
      "grad_norm": 0.36537402868270874,
      "learning_rate": 6.395080707148348e-05,
      "loss": 0.2059,
      "step": 47000
    },
    {
      "epoch": 72.9646697388633,
      "grad_norm": 0.628116250038147,
      "learning_rate": 6.356648731744812e-05,
      "loss": 0.2054,
      "step": 47500
    },
    {
      "epoch": 73.73271889400921,
      "grad_norm": 0.5084418058395386,
      "learning_rate": 6.318216756341276e-05,
      "loss": 0.2048,
      "step": 48000
    },
    {
      "epoch": 74.50076804915514,
      "grad_norm": 0.26685428619384766,
      "learning_rate": 6.27978478093774e-05,
      "loss": 0.2045,
      "step": 48500
    },
    {
      "epoch": 75.26881720430107,
      "grad_norm": 0.44431576132774353,
      "learning_rate": 6.241352805534205e-05,
      "loss": 0.2044,
      "step": 49000
    },
    {
      "epoch": 76.036866359447,
      "grad_norm": 0.1965368539094925,
      "learning_rate": 6.202920830130669e-05,
      "loss": 0.2041,
      "step": 49500
    },
    {
      "epoch": 76.80491551459293,
      "grad_norm": 0.3227885067462921,
      "learning_rate": 6.164488854727133e-05,
      "loss": 0.2038,
      "step": 50000
    },
    {
      "epoch": 77.57296466973887,
      "grad_norm": 0.5789133310317993,
      "learning_rate": 6.126056879323597e-05,
      "loss": 0.2033,
      "step": 50500
    },
    {
      "epoch": 78.3410138248848,
      "grad_norm": 0.39726266264915466,
      "learning_rate": 6.0876249039200616e-05,
      "loss": 0.2031,
      "step": 51000
    },
    {
      "epoch": 79.10906298003073,
      "grad_norm": 0.37213295698165894,
      "learning_rate": 6.049192928516526e-05,
      "loss": 0.203,
      "step": 51500
    },
    {
      "epoch": 79.87711213517665,
      "grad_norm": 0.3548661172389984,
      "learning_rate": 6.010760953112991e-05,
      "loss": 0.2028,
      "step": 52000
    },
    {
      "epoch": 80.64516129032258,
      "grad_norm": 0.2945590615272522,
      "learning_rate": 5.972328977709455e-05,
      "loss": 0.2025,
      "step": 52500
    },
    {
      "epoch": 81.41321044546851,
      "grad_norm": 0.3829882740974426,
      "learning_rate": 5.933897002305919e-05,
      "loss": 0.2022,
      "step": 53000
    },
    {
      "epoch": 82.18125960061444,
      "grad_norm": 0.460611492395401,
      "learning_rate": 5.895465026902383e-05,
      "loss": 0.2017,
      "step": 53500
    },
    {
      "epoch": 82.94930875576037,
      "grad_norm": 0.5607069134712219,
      "learning_rate": 5.857033051498847e-05,
      "loss": 0.2016,
      "step": 54000
    },
    {
      "epoch": 83.7173579109063,
      "grad_norm": 0.30353012681007385,
      "learning_rate": 5.818601076095311e-05,
      "loss": 0.2011,
      "step": 54500
    },
    {
      "epoch": 84.48540706605223,
      "grad_norm": 0.5247812867164612,
      "learning_rate": 5.780169100691776e-05,
      "loss": 0.2013,
      "step": 55000
    },
    {
      "epoch": 85.25345622119816,
      "grad_norm": 0.6242311000823975,
      "learning_rate": 5.74173712528824e-05,
      "loss": 0.201,
      "step": 55500
    },
    {
      "epoch": 86.02150537634408,
      "grad_norm": 0.29166972637176514,
      "learning_rate": 5.7033051498847044e-05,
      "loss": 0.2005,
      "step": 56000
    },
    {
      "epoch": 86.78955453149001,
      "grad_norm": 0.2698688507080078,
      "learning_rate": 5.6648731744811686e-05,
      "loss": 0.2002,
      "step": 56500
    },
    {
      "epoch": 87.55760368663594,
      "grad_norm": 0.4123880863189697,
      "learning_rate": 5.6264411990776335e-05,
      "loss": 0.2,
      "step": 57000
    },
    {
      "epoch": 88.32565284178187,
      "grad_norm": 0.3436425030231476,
      "learning_rate": 5.5880092236740964e-05,
      "loss": 0.2002,
      "step": 57500
    },
    {
      "epoch": 89.0937019969278,
      "grad_norm": 0.5907111167907715,
      "learning_rate": 5.549577248270561e-05,
      "loss": 0.1994,
      "step": 58000
    },
    {
      "epoch": 89.86175115207374,
      "grad_norm": 0.2949431836605072,
      "learning_rate": 5.5111452728670255e-05,
      "loss": 0.1996,
      "step": 58500
    },
    {
      "epoch": 90.62980030721967,
      "grad_norm": 0.4157349467277527,
      "learning_rate": 5.47271329746349e-05,
      "loss": 0.199,
      "step": 59000
    },
    {
      "epoch": 91.39784946236558,
      "grad_norm": 0.3436663746833801,
      "learning_rate": 5.434281322059954e-05,
      "loss": 0.199,
      "step": 59500
    },
    {
      "epoch": 92.16589861751152,
      "grad_norm": 0.6868813037872314,
      "learning_rate": 5.395849346656419e-05,
      "loss": 0.1985,
      "step": 60000
    },
    {
      "epoch": 92.93394777265745,
      "grad_norm": 0.40067124366760254,
      "learning_rate": 5.357417371252883e-05,
      "loss": 0.1984,
      "step": 60500
    },
    {
      "epoch": 93.70199692780338,
      "grad_norm": 0.4408203661441803,
      "learning_rate": 5.318985395849347e-05,
      "loss": 0.1981,
      "step": 61000
    },
    {
      "epoch": 94.47004608294931,
      "grad_norm": 0.4284088909626007,
      "learning_rate": 5.280553420445811e-05,
      "loss": 0.198,
      "step": 61500
    },
    {
      "epoch": 95.23809523809524,
      "grad_norm": 0.5029202699661255,
      "learning_rate": 5.242121445042275e-05,
      "loss": 0.1977,
      "step": 62000
    },
    {
      "epoch": 96.00614439324117,
      "grad_norm": 0.39703232049942017,
      "learning_rate": 5.203689469638739e-05,
      "loss": 0.1976,
      "step": 62500
    },
    {
      "epoch": 96.7741935483871,
      "grad_norm": 0.3953171372413635,
      "learning_rate": 5.165257494235204e-05,
      "loss": 0.1972,
      "step": 63000
    },
    {
      "epoch": 97.54224270353302,
      "grad_norm": 0.3029524087905884,
      "learning_rate": 5.126825518831668e-05,
      "loss": 0.197,
      "step": 63500
    },
    {
      "epoch": 98.31029185867895,
      "grad_norm": 0.5104950070381165,
      "learning_rate": 5.0883935434281325e-05,
      "loss": 0.1969,
      "step": 64000
    },
    {
      "epoch": 99.07834101382488,
      "grad_norm": 0.4797203540802002,
      "learning_rate": 5.049961568024597e-05,
      "loss": 0.1967,
      "step": 64500
    },
    {
      "epoch": 99.84639016897081,
      "grad_norm": 0.22930464148521423,
      "learning_rate": 5.0115295926210617e-05,
      "loss": 0.1962,
      "step": 65000
    },
    {
      "epoch": 100.61443932411674,
      "grad_norm": 0.43303754925727844,
      "learning_rate": 4.973097617217525e-05,
      "loss": 0.1962,
      "step": 65500
    },
    {
      "epoch": 101.38248847926268,
      "grad_norm": 0.3447020947933197,
      "learning_rate": 4.9346656418139894e-05,
      "loss": 0.196,
      "step": 66000
    },
    {
      "epoch": 102.15053763440861,
      "grad_norm": 0.5739824771881104,
      "learning_rate": 4.8962336664104536e-05,
      "loss": 0.1957,
      "step": 66500
    },
    {
      "epoch": 102.91858678955452,
      "grad_norm": 0.3261962831020355,
      "learning_rate": 4.857801691006918e-05,
      "loss": 0.1954,
      "step": 67000
    },
    {
      "epoch": 103.68663594470046,
      "grad_norm": 0.5920395851135254,
      "learning_rate": 4.819369715603382e-05,
      "loss": 0.1953,
      "step": 67500
    },
    {
      "epoch": 104.45468509984639,
      "grad_norm": 0.4828518331050873,
      "learning_rate": 4.780937740199847e-05,
      "loss": 0.1951,
      "step": 68000
    },
    {
      "epoch": 105.22273425499232,
      "grad_norm": 0.2594571113586426,
      "learning_rate": 4.7425057647963105e-05,
      "loss": 0.195,
      "step": 68500
    },
    {
      "epoch": 105.99078341013825,
      "grad_norm": 0.35385510325431824,
      "learning_rate": 4.704073789392775e-05,
      "loss": 0.1947,
      "step": 69000
    },
    {
      "epoch": 106.75883256528418,
      "grad_norm": 0.4363006353378296,
      "learning_rate": 4.6656418139892396e-05,
      "loss": 0.1948,
      "step": 69500
    },
    {
      "epoch": 107.52688172043011,
      "grad_norm": 0.5565077662467957,
      "learning_rate": 4.627209838585704e-05,
      "loss": 0.1945,
      "step": 70000
    },
    {
      "epoch": 108.29493087557604,
      "grad_norm": 0.7423551082611084,
      "learning_rate": 4.588777863182167e-05,
      "loss": 0.194,
      "step": 70500
    },
    {
      "epoch": 109.06298003072196,
      "grad_norm": 0.2806924283504486,
      "learning_rate": 4.550345887778632e-05,
      "loss": 0.1939,
      "step": 71000
    },
    {
      "epoch": 109.83102918586789,
      "grad_norm": 0.28660523891448975,
      "learning_rate": 4.5119139123750964e-05,
      "loss": 0.1937,
      "step": 71500
    },
    {
      "epoch": 110.59907834101382,
      "grad_norm": 0.5239590406417847,
      "learning_rate": 4.4734819369715607e-05,
      "loss": 0.1937,
      "step": 72000
    },
    {
      "epoch": 111.36712749615975,
      "grad_norm": 0.60733562707901,
      "learning_rate": 4.435049961568025e-05,
      "loss": 0.1935,
      "step": 72500
    },
    {
      "epoch": 112.13517665130568,
      "grad_norm": 0.6082967519760132,
      "learning_rate": 4.396617986164489e-05,
      "loss": 0.1934,
      "step": 73000
    },
    {
      "epoch": 112.90322580645162,
      "grad_norm": 0.6446011662483215,
      "learning_rate": 4.358186010760953e-05,
      "loss": 0.1932,
      "step": 73500
    },
    {
      "epoch": 113.67127496159755,
      "grad_norm": 0.3841666281223297,
      "learning_rate": 4.3197540353574175e-05,
      "loss": 0.1928,
      "step": 74000
    },
    {
      "epoch": 114.43932411674348,
      "grad_norm": 0.7139571309089661,
      "learning_rate": 4.281322059953882e-05,
      "loss": 0.1927,
      "step": 74500
    },
    {
      "epoch": 115.2073732718894,
      "grad_norm": 0.4614297151565552,
      "learning_rate": 4.242890084550346e-05,
      "loss": 0.1926,
      "step": 75000
    },
    {
      "epoch": 115.97542242703533,
      "grad_norm": 0.33099251985549927,
      "learning_rate": 4.20445810914681e-05,
      "loss": 0.1926,
      "step": 75500
    },
    {
      "epoch": 116.74347158218126,
      "grad_norm": 0.64556485414505,
      "learning_rate": 4.1660261337432744e-05,
      "loss": 0.1923,
      "step": 76000
    },
    {
      "epoch": 117.51152073732719,
      "grad_norm": 0.7405653595924377,
      "learning_rate": 4.1275941583397386e-05,
      "loss": 0.1921,
      "step": 76500
    },
    {
      "epoch": 118.27956989247312,
      "grad_norm": 0.47042909264564514,
      "learning_rate": 4.089162182936203e-05,
      "loss": 0.1923,
      "step": 77000
    },
    {
      "epoch": 119.04761904761905,
      "grad_norm": 0.512295126914978,
      "learning_rate": 4.050730207532668e-05,
      "loss": 0.1918,
      "step": 77500
    },
    {
      "epoch": 119.81566820276498,
      "grad_norm": 0.3220251798629761,
      "learning_rate": 4.012298232129131e-05,
      "loss": 0.1917,
      "step": 78000
    },
    {
      "epoch": 120.5837173579109,
      "grad_norm": 0.4726102948188782,
      "learning_rate": 3.9738662567255955e-05,
      "loss": 0.1917,
      "step": 78500
    },
    {
      "epoch": 121.35176651305683,
      "grad_norm": 0.8034325838088989,
      "learning_rate": 3.9354342813220603e-05,
      "loss": 0.1916,
      "step": 79000
    },
    {
      "epoch": 122.11981566820276,
      "grad_norm": 0.6048251390457153,
      "learning_rate": 3.8970023059185246e-05,
      "loss": 0.1912,
      "step": 79500
    },
    {
      "epoch": 122.88786482334869,
      "grad_norm": 0.3367086350917816,
      "learning_rate": 3.858570330514988e-05,
      "loss": 0.1913,
      "step": 80000
    },
    {
      "epoch": 123.65591397849462,
      "grad_norm": 0.15986961126327515,
      "learning_rate": 3.820138355111453e-05,
      "loss": 0.191,
      "step": 80500
    },
    {
      "epoch": 124.42396313364056,
      "grad_norm": 0.3735271692276001,
      "learning_rate": 3.781706379707917e-05,
      "loss": 0.1911,
      "step": 81000
    },
    {
      "epoch": 125.19201228878649,
      "grad_norm": 0.6673787832260132,
      "learning_rate": 3.7432744043043814e-05,
      "loss": 0.1907,
      "step": 81500
    },
    {
      "epoch": 125.96006144393242,
      "grad_norm": 0.2786695063114166,
      "learning_rate": 3.7048424289008456e-05,
      "loss": 0.191,
      "step": 82000
    },
    {
      "epoch": 126.72811059907833,
      "grad_norm": 0.302481085062027,
      "learning_rate": 3.66641045349731e-05,
      "loss": 0.1902,
      "step": 82500
    },
    {
      "epoch": 127.49615975422427,
      "grad_norm": 0.6323248744010925,
      "learning_rate": 3.627978478093774e-05,
      "loss": 0.1907,
      "step": 83000
    },
    {
      "epoch": 128.2642089093702,
      "grad_norm": 0.34864285588264465,
      "learning_rate": 3.589546502690239e-05,
      "loss": 0.1906,
      "step": 83500
    },
    {
      "epoch": 129.03225806451613,
      "grad_norm": 0.23872970044612885,
      "learning_rate": 3.5511145272867025e-05,
      "loss": 0.1904,
      "step": 84000
    },
    {
      "epoch": 129.80030721966205,
      "grad_norm": 0.3236793875694275,
      "learning_rate": 3.512682551883167e-05,
      "loss": 0.1903,
      "step": 84500
    },
    {
      "epoch": 130.568356374808,
      "grad_norm": 0.190851628780365,
      "learning_rate": 3.474250576479631e-05,
      "loss": 0.1899,
      "step": 85000
    },
    {
      "epoch": 131.3364055299539,
      "grad_norm": 0.3346264660358429,
      "learning_rate": 3.435818601076096e-05,
      "loss": 0.19,
      "step": 85500
    },
    {
      "epoch": 132.10445468509985,
      "grad_norm": 0.3258439302444458,
      "learning_rate": 3.3973866256725594e-05,
      "loss": 0.1898,
      "step": 86000
    },
    {
      "epoch": 132.87250384024577,
      "grad_norm": 0.20085635781288147,
      "learning_rate": 3.3589546502690236e-05,
      "loss": 0.1899,
      "step": 86500
    },
    {
      "epoch": 133.64055299539172,
      "grad_norm": 0.7650819420814514,
      "learning_rate": 3.3205226748654885e-05,
      "loss": 0.1894,
      "step": 87000
    },
    {
      "epoch": 134.40860215053763,
      "grad_norm": 0.298656702041626,
      "learning_rate": 3.282090699461953e-05,
      "loss": 0.1901,
      "step": 87500
    },
    {
      "epoch": 135.17665130568358,
      "grad_norm": 0.39349210262298584,
      "learning_rate": 3.243658724058416e-05,
      "loss": 0.1891,
      "step": 88000
    },
    {
      "epoch": 135.9447004608295,
      "grad_norm": 0.43892839550971985,
      "learning_rate": 3.205226748654881e-05,
      "loss": 0.1895,
      "step": 88500
    },
    {
      "epoch": 136.7127496159754,
      "grad_norm": 0.5056846141815186,
      "learning_rate": 3.166794773251345e-05,
      "loss": 0.1893,
      "step": 89000
    },
    {
      "epoch": 137.48079877112136,
      "grad_norm": 0.35295939445495605,
      "learning_rate": 3.1283627978478095e-05,
      "loss": 0.1894,
      "step": 89500
    },
    {
      "epoch": 138.24884792626727,
      "grad_norm": 0.41177210211753845,
      "learning_rate": 3.089930822444274e-05,
      "loss": 0.1889,
      "step": 90000
    },
    {
      "epoch": 139.01689708141322,
      "grad_norm": 0.20682258903980255,
      "learning_rate": 3.051498847040738e-05,
      "loss": 0.189,
      "step": 90500
    },
    {
      "epoch": 139.78494623655914,
      "grad_norm": 0.39329278469085693,
      "learning_rate": 3.0130668716372025e-05,
      "loss": 0.1889,
      "step": 91000
    },
    {
      "epoch": 140.55299539170508,
      "grad_norm": 0.5195579528808594,
      "learning_rate": 2.9746348962336667e-05,
      "loss": 0.1888,
      "step": 91500
    },
    {
      "epoch": 141.321044546851,
      "grad_norm": 0.2655203342437744,
      "learning_rate": 2.9362029208301306e-05,
      "loss": 0.1891,
      "step": 92000
    },
    {
      "epoch": 142.08909370199692,
      "grad_norm": 0.5504984855651855,
      "learning_rate": 2.897770945426595e-05,
      "loss": 0.1886,
      "step": 92500
    },
    {
      "epoch": 142.85714285714286,
      "grad_norm": 0.43542394042015076,
      "learning_rate": 2.8593389700230594e-05,
      "loss": 0.1883,
      "step": 93000
    },
    {
      "epoch": 143.62519201228878,
      "grad_norm": 0.3956142067909241,
      "learning_rate": 2.820906994619524e-05,
      "loss": 0.1887,
      "step": 93500
    },
    {
      "epoch": 144.39324116743472,
      "grad_norm": 0.44743582606315613,
      "learning_rate": 2.7824750192159875e-05,
      "loss": 0.1882,
      "step": 94000
    },
    {
      "epoch": 145.16129032258064,
      "grad_norm": 0.4342259168624878,
      "learning_rate": 2.744043043812452e-05,
      "loss": 0.1884,
      "step": 94500
    },
    {
      "epoch": 145.9293394777266,
      "grad_norm": 0.24675676226615906,
      "learning_rate": 2.7056110684089166e-05,
      "loss": 0.1883,
      "step": 95000
    },
    {
      "epoch": 146.6973886328725,
      "grad_norm": 0.2494688183069229,
      "learning_rate": 2.6671790930053808e-05,
      "loss": 0.1881,
      "step": 95500
    },
    {
      "epoch": 147.46543778801842,
      "grad_norm": 0.2593018412590027,
      "learning_rate": 2.6287471176018447e-05,
      "loss": 0.1884,
      "step": 96000
    },
    {
      "epoch": 148.23348694316437,
      "grad_norm": 0.24615158140659332,
      "learning_rate": 2.5903151421983092e-05,
      "loss": 0.1878,
      "step": 96500
    },
    {
      "epoch": 149.00153609831028,
      "grad_norm": 0.5609015822410583,
      "learning_rate": 2.5518831667947734e-05,
      "loss": 0.1879,
      "step": 97000
    },
    {
      "epoch": 149.76958525345623,
      "grad_norm": 0.46976444125175476,
      "learning_rate": 2.513451191391238e-05,
      "loss": 0.1878,
      "step": 97500
    },
    {
      "epoch": 150.53763440860214,
      "grad_norm": 0.41457775235176086,
      "learning_rate": 2.475019215987702e-05,
      "loss": 0.1878,
      "step": 98000
    },
    {
      "epoch": 151.3056835637481,
      "grad_norm": 0.5738101005554199,
      "learning_rate": 2.436587240584166e-05,
      "loss": 0.1879,
      "step": 98500
    },
    {
      "epoch": 152.073732718894,
      "grad_norm": 0.5524709820747375,
      "learning_rate": 2.3981552651806306e-05,
      "loss": 0.1875,
      "step": 99000
    },
    {
      "epoch": 152.84178187403995,
      "grad_norm": 0.4019468128681183,
      "learning_rate": 2.3597232897770945e-05,
      "loss": 0.1877,
      "step": 99500
    },
    {
      "epoch": 153.60983102918587,
      "grad_norm": 0.24079498648643494,
      "learning_rate": 2.321291314373559e-05,
      "loss": 0.1875,
      "step": 100000
    },
    {
      "epoch": 154.3778801843318,
      "grad_norm": 0.3516499698162079,
      "learning_rate": 2.2828593389700233e-05,
      "loss": 0.1874,
      "step": 100500
    },
    {
      "epoch": 155.14592933947773,
      "grad_norm": 0.3051917850971222,
      "learning_rate": 2.2444273635664875e-05,
      "loss": 0.1875,
      "step": 101000
    },
    {
      "epoch": 155.91397849462365,
      "grad_norm": 0.5983681678771973,
      "learning_rate": 2.2059953881629517e-05,
      "loss": 0.1874,
      "step": 101500
    },
    {
      "epoch": 156.6820276497696,
      "grad_norm": 0.46243923902511597,
      "learning_rate": 2.167563412759416e-05,
      "loss": 0.1874,
      "step": 102000
    },
    {
      "epoch": 157.4500768049155,
      "grad_norm": 0.23260430991649628,
      "learning_rate": 2.12913143735588e-05,
      "loss": 0.187,
      "step": 102500
    },
    {
      "epoch": 158.21812596006146,
      "grad_norm": 0.8142558932304382,
      "learning_rate": 2.0906994619523447e-05,
      "loss": 0.1868,
      "step": 103000
    },
    {
      "epoch": 158.98617511520737,
      "grad_norm": 0.38539838790893555,
      "learning_rate": 2.0522674865488086e-05,
      "loss": 0.1874,
      "step": 103500
    },
    {
      "epoch": 159.7542242703533,
      "grad_norm": 0.39877960085868835,
      "learning_rate": 2.013835511145273e-05,
      "loss": 0.1867,
      "step": 104000
    },
    {
      "epoch": 160.52227342549924,
      "grad_norm": 0.49940118193626404,
      "learning_rate": 1.9754035357417373e-05,
      "loss": 0.1874,
      "step": 104500
    },
    {
      "epoch": 161.29032258064515,
      "grad_norm": 0.357841819524765,
      "learning_rate": 1.9369715603382015e-05,
      "loss": 0.1869,
      "step": 105000
    },
    {
      "epoch": 162.0583717357911,
      "grad_norm": 0.19489136338233948,
      "learning_rate": 1.8985395849346658e-05,
      "loss": 0.1867,
      "step": 105500
    },
    {
      "epoch": 162.82642089093702,
      "grad_norm": 0.40615877509117126,
      "learning_rate": 1.86010760953113e-05,
      "loss": 0.1868,
      "step": 106000
    },
    {
      "epoch": 163.59447004608296,
      "grad_norm": 0.3619631826877594,
      "learning_rate": 1.8216756341275942e-05,
      "loss": 0.1868,
      "step": 106500
    },
    {
      "epoch": 164.36251920122888,
      "grad_norm": 0.2683287560939789,
      "learning_rate": 1.7832436587240587e-05,
      "loss": 0.1865,
      "step": 107000
    },
    {
      "epoch": 165.1305683563748,
      "grad_norm": 0.42695990204811096,
      "learning_rate": 1.7448116833205226e-05,
      "loss": 0.1867,
      "step": 107500
    },
    {
      "epoch": 165.89861751152074,
      "grad_norm": 0.5266474485397339,
      "learning_rate": 1.7063797079169872e-05,
      "loss": 0.1865,
      "step": 108000
    },
    {
      "epoch": 166.66666666666666,
      "grad_norm": 0.4693443775177002,
      "learning_rate": 1.6679477325134514e-05,
      "loss": 0.1863,
      "step": 108500
    },
    {
      "epoch": 167.4347158218126,
      "grad_norm": 0.35771608352661133,
      "learning_rate": 1.6295157571099156e-05,
      "loss": 0.187,
      "step": 109000
    },
    {
      "epoch": 168.20276497695852,
      "grad_norm": 0.26833677291870117,
      "learning_rate": 1.5910837817063798e-05,
      "loss": 0.1859,
      "step": 109500
    },
    {
      "epoch": 168.97081413210446,
      "grad_norm": 0.5011033415794373,
      "learning_rate": 1.552651806302844e-05,
      "loss": 0.1864,
      "step": 110000
    },
    {
      "epoch": 169.73886328725038,
      "grad_norm": 0.39467135071754456,
      "learning_rate": 1.5142198308993082e-05,
      "loss": 0.1865,
      "step": 110500
    },
    {
      "epoch": 170.50691244239633,
      "grad_norm": 0.13978560268878937,
      "learning_rate": 1.4757878554957725e-05,
      "loss": 0.1862,
      "step": 111000
    },
    {
      "epoch": 171.27496159754224,
      "grad_norm": 0.6281638145446777,
      "learning_rate": 1.4373558800922368e-05,
      "loss": 0.1864,
      "step": 111500
    },
    {
      "epoch": 172.04301075268816,
      "grad_norm": 0.22408118844032288,
      "learning_rate": 1.3989239046887009e-05,
      "loss": 0.1861,
      "step": 112000
    },
    {
      "epoch": 172.8110599078341,
      "grad_norm": 0.22932450473308563,
      "learning_rate": 1.3604919292851653e-05,
      "loss": 0.1864,
      "step": 112500
    },
    {
      "epoch": 173.57910906298002,
      "grad_norm": 0.2898138761520386,
      "learning_rate": 1.3220599538816295e-05,
      "loss": 0.1859,
      "step": 113000
    },
    {
      "epoch": 174.34715821812597,
      "grad_norm": 0.41245850920677185,
      "learning_rate": 1.2836279784780939e-05,
      "loss": 0.1863,
      "step": 113500
    },
    {
      "epoch": 175.1152073732719,
      "grad_norm": 0.30145999789237976,
      "learning_rate": 1.2451960030745581e-05,
      "loss": 0.1859,
      "step": 114000
    },
    {
      "epoch": 175.88325652841783,
      "grad_norm": 0.28621312975883484,
      "learning_rate": 1.2067640276710223e-05,
      "loss": 0.1862,
      "step": 114500
    },
    {
      "epoch": 176.65130568356375,
      "grad_norm": 0.24722914397716522,
      "learning_rate": 1.1683320522674867e-05,
      "loss": 0.1856,
      "step": 115000
    },
    {
      "epoch": 177.41935483870967,
      "grad_norm": 0.14023545384407043,
      "learning_rate": 1.1299000768639509e-05,
      "loss": 0.1862,
      "step": 115500
    },
    {
      "epoch": 178.1874039938556,
      "grad_norm": 0.4352112412452698,
      "learning_rate": 1.0914681014604151e-05,
      "loss": 0.1859,
      "step": 116000
    },
    {
      "epoch": 178.95545314900153,
      "grad_norm": 0.3653703033924103,
      "learning_rate": 1.0530361260568793e-05,
      "loss": 0.1858,
      "step": 116500
    },
    {
      "epoch": 179.72350230414747,
      "grad_norm": 0.2620934545993805,
      "learning_rate": 1.0146041506533437e-05,
      "loss": 0.1859,
      "step": 117000
    },
    {
      "epoch": 180.4915514592934,
      "grad_norm": 0.2967815399169922,
      "learning_rate": 9.76172175249808e-06,
      "loss": 0.1855,
      "step": 117500
    },
    {
      "epoch": 181.25960061443934,
      "grad_norm": 0.4386144280433655,
      "learning_rate": 9.377401998462721e-06,
      "loss": 0.1861,
      "step": 118000
    },
    {
      "epoch": 182.02764976958525,
      "grad_norm": 0.3703664243221283,
      "learning_rate": 8.993082244427364e-06,
      "loss": 0.1859,
      "step": 118500
    },
    {
      "epoch": 182.79569892473117,
      "grad_norm": 0.15494902431964874,
      "learning_rate": 8.608762490392007e-06,
      "loss": 0.1856,
      "step": 119000
    },
    {
      "epoch": 183.56374807987712,
      "grad_norm": 0.6287676692008972,
      "learning_rate": 8.22444273635665e-06,
      "loss": 0.186,
      "step": 119500
    },
    {
      "epoch": 184.33179723502303,
      "grad_norm": 0.3242611289024353,
      "learning_rate": 7.840122982321292e-06,
      "loss": 0.1856,
      "step": 120000
    },
    {
      "epoch": 185.09984639016898,
      "grad_norm": 0.33616721630096436,
      "learning_rate": 7.455803228285935e-06,
      "loss": 0.1853,
      "step": 120500
    },
    {
      "epoch": 185.8678955453149,
      "grad_norm": 0.17082254588603973,
      "learning_rate": 7.071483474250577e-06,
      "loss": 0.1859,
      "step": 121000
    },
    {
      "epoch": 186.63594470046084,
      "grad_norm": 0.3997124433517456,
      "learning_rate": 6.68716372021522e-06,
      "loss": 0.1855,
      "step": 121500
    },
    {
      "epoch": 187.40399385560676,
      "grad_norm": 0.2965472936630249,
      "learning_rate": 6.302843966179863e-06,
      "loss": 0.1861,
      "step": 122000
    },
    {
      "epoch": 188.1720430107527,
      "grad_norm": 0.19931183755397797,
      "learning_rate": 5.918524212144504e-06,
      "loss": 0.1852,
      "step": 122500
    },
    {
      "epoch": 188.94009216589862,
      "grad_norm": 0.34783536195755005,
      "learning_rate": 5.534204458109147e-06,
      "loss": 0.1856,
      "step": 123000
    },
    {
      "epoch": 189.70814132104454,
      "grad_norm": 0.2660945951938629,
      "learning_rate": 5.149884704073789e-06,
      "loss": 0.1856,
      "step": 123500
    },
    {
      "epoch": 190.47619047619048,
      "grad_norm": 0.25372934341430664,
      "learning_rate": 4.765564950038432e-06,
      "loss": 0.1855,
      "step": 124000
    },
    {
      "epoch": 191.2442396313364,
      "grad_norm": 0.32931044697761536,
      "learning_rate": 4.3812451960030744e-06,
      "loss": 0.1852,
      "step": 124500
    },
    {
      "epoch": 192.01228878648234,
      "grad_norm": 0.40331923961639404,
      "learning_rate": 3.9969254419677174e-06,
      "loss": 0.1857,
      "step": 125000
    },
    {
      "epoch": 192.78033794162826,
      "grad_norm": 0.44256341457366943,
      "learning_rate": 3.61260568793236e-06,
      "loss": 0.1853,
      "step": 125500
    },
    {
      "epoch": 193.5483870967742,
      "grad_norm": 0.2301529496908188,
      "learning_rate": 3.2282859338970026e-06,
      "loss": 0.1855,
      "step": 126000
    },
    {
      "epoch": 194.31643625192012,
      "grad_norm": 0.36991679668426514,
      "learning_rate": 2.843966179861645e-06,
      "loss": 0.1857,
      "step": 126500
    },
    {
      "epoch": 195.08448540706604,
      "grad_norm": 0.28805971145629883,
      "learning_rate": 2.4596464258262877e-06,
      "loss": 0.1855,
      "step": 127000
    },
    {
      "epoch": 195.85253456221199,
      "grad_norm": 0.3199399411678314,
      "learning_rate": 2.0753266717909303e-06,
      "loss": 0.1853,
      "step": 127500
    },
    {
      "epoch": 196.6205837173579,
      "grad_norm": 0.2360900193452835,
      "learning_rate": 1.6910069177555729e-06,
      "loss": 0.1851,
      "step": 128000
    },
    {
      "epoch": 197.38863287250385,
      "grad_norm": 0.3315798342227936,
      "learning_rate": 1.3066871637202154e-06,
      "loss": 0.1855,
      "step": 128500
    },
    {
      "epoch": 198.15668202764977,
      "grad_norm": 0.3576822280883789,
      "learning_rate": 9.223674096848578e-07,
      "loss": 0.1855,
      "step": 129000
    },
    {
      "epoch": 198.9247311827957,
      "grad_norm": 0.45139288902282715,
      "learning_rate": 5.380476556495005e-07,
      "loss": 0.1853,
      "step": 129500
    },
    {
      "epoch": 199.69278033794163,
      "grad_norm": 0.22917477786540985,
      "learning_rate": 1.5372790161414298e-07,
      "loss": 0.1855,
      "step": 130000
    },
    {
      "epoch": 200.46082949308754,
      "grad_norm": 0.22229309380054474,
      "learning_rate": 3.319672131147541e-05,
      "loss": 0.1855,
      "step": 130500
    },
    {
      "epoch": 201.2288786482335,
      "grad_norm": 0.5273635983467102,
      "learning_rate": 3.2940573770491805e-05,
      "loss": 0.1858,
      "step": 131000
    },
    {
      "epoch": 201.9969278033794,
      "grad_norm": 0.4507921040058136,
      "learning_rate": 3.26844262295082e-05,
      "loss": 0.1853,
      "step": 131500
    },
    {
      "epoch": 202.76497695852535,
      "grad_norm": 0.4406834542751312,
      "learning_rate": 3.242827868852459e-05,
      "loss": 0.1856,
      "step": 132000
    },
    {
      "epoch": 203.53302611367127,
      "grad_norm": 0.30668938159942627,
      "learning_rate": 3.2172131147540984e-05,
      "loss": 0.1852,
      "step": 132500
    },
    {
      "epoch": 204.30107526881721,
      "grad_norm": 0.2739531695842743,
      "learning_rate": 3.191598360655738e-05,
      "loss": 0.1851,
      "step": 133000
    },
    {
      "epoch": 205.06912442396313,
      "grad_norm": 0.5545360445976257,
      "learning_rate": 3.165983606557377e-05,
      "loss": 0.185,
      "step": 133500
    },
    {
      "epoch": 205.83717357910908,
      "grad_norm": 0.40940606594085693,
      "learning_rate": 3.1403688524590163e-05,
      "loss": 0.185,
      "step": 134000
    },
    {
      "epoch": 206.605222734255,
      "grad_norm": 0.5180210471153259,
      "learning_rate": 3.114754098360656e-05,
      "loss": 0.185,
      "step": 134500
    },
    {
      "epoch": 207.3732718894009,
      "grad_norm": 0.29124128818511963,
      "learning_rate": 3.089139344262295e-05,
      "loss": 0.1849,
      "step": 135000
    },
    {
      "epoch": 208.14132104454686,
      "grad_norm": 0.31951773166656494,
      "learning_rate": 3.063524590163935e-05,
      "loss": 0.185,
      "step": 135500
    },
    {
      "epoch": 208.90937019969277,
      "grad_norm": 0.30287179350852966,
      "learning_rate": 3.037909836065574e-05,
      "loss": 0.1849,
      "step": 136000
    },
    {
      "epoch": 209.67741935483872,
      "grad_norm": 0.5575536489486694,
      "learning_rate": 3.012295081967213e-05,
      "loss": 0.1847,
      "step": 136500
    },
    {
      "epoch": 210.44546850998464,
      "grad_norm": 0.5658696889877319,
      "learning_rate": 2.9866803278688525e-05,
      "loss": 0.1843,
      "step": 137000
    },
    {
      "epoch": 211.21351766513058,
      "grad_norm": 0.506597101688385,
      "learning_rate": 2.961065573770492e-05,
      "loss": 0.1849,
      "step": 137500
    },
    {
      "epoch": 211.9815668202765,
      "grad_norm": 0.6487391591072083,
      "learning_rate": 2.9354508196721315e-05,
      "loss": 0.1843,
      "step": 138000
    },
    {
      "epoch": 212.74961597542242,
      "grad_norm": 0.6235557198524475,
      "learning_rate": 2.9098360655737705e-05,
      "loss": 0.1842,
      "step": 138500
    },
    {
      "epoch": 213.51766513056836,
      "grad_norm": 0.5930439829826355,
      "learning_rate": 2.88422131147541e-05,
      "loss": 0.1846,
      "step": 139000
    },
    {
      "epoch": 214.28571428571428,
      "grad_norm": 0.3657468259334564,
      "learning_rate": 2.8586065573770494e-05,
      "loss": 0.1843,
      "step": 139500
    },
    {
      "epoch": 215.05376344086022,
      "grad_norm": 0.5183390974998474,
      "learning_rate": 2.8329918032786884e-05,
      "loss": 0.1842,
      "step": 140000
    },
    {
      "epoch": 215.82181259600614,
      "grad_norm": 0.3377949893474579,
      "learning_rate": 2.807377049180328e-05,
      "loss": 0.1843,
      "step": 140500
    },
    {
      "epoch": 216.58986175115209,
      "grad_norm": 0.39462146162986755,
      "learning_rate": 2.781762295081967e-05,
      "loss": 0.1839,
      "step": 141000
    },
    {
      "epoch": 217.357910906298,
      "grad_norm": 0.2875787913799286,
      "learning_rate": 2.7561475409836067e-05,
      "loss": 0.1838,
      "step": 141500
    },
    {
      "epoch": 218.12596006144392,
      "grad_norm": 0.35458430647850037,
      "learning_rate": 2.730532786885246e-05,
      "loss": 0.1842,
      "step": 142000
    },
    {
      "epoch": 218.89400921658986,
      "grad_norm": 0.2902836799621582,
      "learning_rate": 2.7049180327868856e-05,
      "loss": 0.184,
      "step": 142500
    },
    {
      "epoch": 219.66205837173578,
      "grad_norm": 0.34744158387184143,
      "learning_rate": 2.6793032786885246e-05,
      "loss": 0.184,
      "step": 143000
    },
    {
      "epoch": 220.43010752688173,
      "grad_norm": 0.1859753429889679,
      "learning_rate": 2.653688524590164e-05,
      "loss": 0.1838,
      "step": 143500
    },
    {
      "epoch": 221.19815668202764,
      "grad_norm": 0.29040083289146423,
      "learning_rate": 2.6280737704918036e-05,
      "loss": 0.1835,
      "step": 144000
    },
    {
      "epoch": 221.9662058371736,
      "grad_norm": 0.27123066782951355,
      "learning_rate": 2.6024590163934425e-05,
      "loss": 0.1837,
      "step": 144500
    },
    {
      "epoch": 222.7342549923195,
      "grad_norm": 0.22611750662326813,
      "learning_rate": 2.5768442622950822e-05,
      "loss": 0.1838,
      "step": 145000
    },
    {
      "epoch": 223.50230414746545,
      "grad_norm": 0.8152838349342346,
      "learning_rate": 2.5512295081967215e-05,
      "loss": 0.1833,
      "step": 145500
    },
    {
      "epoch": 224.27035330261137,
      "grad_norm": 0.5252469182014465,
      "learning_rate": 2.525614754098361e-05,
      "loss": 0.1835,
      "step": 146000
    },
    {
      "epoch": 225.0384024577573,
      "grad_norm": 0.4438016712665558,
      "learning_rate": 2.5e-05,
      "loss": 0.1836,
      "step": 146500
    },
    {
      "epoch": 225.80645161290323,
      "grad_norm": 0.35112348198890686,
      "learning_rate": 2.4743852459016394e-05,
      "loss": 0.1835,
      "step": 147000
    },
    {
      "epoch": 226.57450076804915,
      "grad_norm": 0.500588595867157,
      "learning_rate": 2.4487704918032787e-05,
      "loss": 0.1832,
      "step": 147500
    },
    {
      "epoch": 227.3425499231951,
      "grad_norm": 0.3348895013332367,
      "learning_rate": 2.4231557377049184e-05,
      "loss": 0.1833,
      "step": 148000
    },
    {
      "epoch": 228.110599078341,
      "grad_norm": 0.5771355032920837,
      "learning_rate": 2.3975409836065574e-05,
      "loss": 0.1833,
      "step": 148500
    },
    {
      "epoch": 228.87864823348696,
      "grad_norm": 0.4012832045555115,
      "learning_rate": 2.3719262295081967e-05,
      "loss": 0.1832,
      "step": 149000
    },
    {
      "epoch": 229.64669738863287,
      "grad_norm": 0.505818784236908,
      "learning_rate": 2.346311475409836e-05,
      "loss": 0.1834,
      "step": 149500
    },
    {
      "epoch": 230.4147465437788,
      "grad_norm": 0.6928777098655701,
      "learning_rate": 2.3206967213114756e-05,
      "loss": 0.1829,
      "step": 150000
    },
    {
      "epoch": 231.18279569892474,
      "grad_norm": 0.23463459312915802,
      "learning_rate": 2.295081967213115e-05,
      "loss": 0.1829,
      "step": 150500
    },
    {
      "epoch": 231.95084485407065,
      "grad_norm": 0.3358296751976013,
      "learning_rate": 2.2694672131147543e-05,
      "loss": 0.183,
      "step": 151000
    },
    {
      "epoch": 232.7188940092166,
      "grad_norm": 0.46372872591018677,
      "learning_rate": 2.2438524590163936e-05,
      "loss": 0.1828,
      "step": 151500
    },
    {
      "epoch": 233.48694316436251,
      "grad_norm": 0.45252901315689087,
      "learning_rate": 2.218237704918033e-05,
      "loss": 0.183,
      "step": 152000
    },
    {
      "epoch": 234.25499231950846,
      "grad_norm": 0.5531121492385864,
      "learning_rate": 2.1926229508196722e-05,
      "loss": 0.1826,
      "step": 152500
    },
    {
      "epoch": 235.02304147465438,
      "grad_norm": 0.29560592770576477,
      "learning_rate": 2.1670081967213115e-05,
      "loss": 0.1829,
      "step": 153000
    },
    {
      "epoch": 235.7910906298003,
      "grad_norm": 0.3621024787425995,
      "learning_rate": 2.1413934426229508e-05,
      "loss": 0.1828,
      "step": 153500
    },
    {
      "epoch": 236.55913978494624,
      "grad_norm": 0.5654823780059814,
      "learning_rate": 2.1157786885245905e-05,
      "loss": 0.1827,
      "step": 154000
    },
    {
      "epoch": 237.32718894009216,
      "grad_norm": 0.3153156042098999,
      "learning_rate": 2.0901639344262298e-05,
      "loss": 0.1825,
      "step": 154500
    },
    {
      "epoch": 238.0952380952381,
      "grad_norm": 0.21315895020961761,
      "learning_rate": 2.0645491803278687e-05,
      "loss": 0.1825,
      "step": 155000
    },
    {
      "epoch": 238.86328725038402,
      "grad_norm": 0.3270569145679474,
      "learning_rate": 2.038934426229508e-05,
      "loss": 0.1825,
      "step": 155500
    },
    {
      "epoch": 239.63133640552996,
      "grad_norm": 0.23833586275577545,
      "learning_rate": 2.0133196721311477e-05,
      "loss": 0.1827,
      "step": 156000
    },
    {
      "epoch": 240.39938556067588,
      "grad_norm": 0.500801146030426,
      "learning_rate": 1.987704918032787e-05,
      "loss": 0.1823,
      "step": 156500
    },
    {
      "epoch": 241.16743471582183,
      "grad_norm": 0.4227250814437866,
      "learning_rate": 1.9620901639344263e-05,
      "loss": 0.1827,
      "step": 157000
    },
    {
      "epoch": 241.93548387096774,
      "grad_norm": 0.3942398130893707,
      "learning_rate": 1.9364754098360656e-05,
      "loss": 0.1823,
      "step": 157500
    },
    {
      "epoch": 242.70353302611366,
      "grad_norm": 0.334405779838562,
      "learning_rate": 1.9108606557377053e-05,
      "loss": 0.1824,
      "step": 158000
    },
    {
      "epoch": 243.4715821812596,
      "grad_norm": 0.5654151439666748,
      "learning_rate": 1.8852459016393442e-05,
      "loss": 0.1824,
      "step": 158500
    },
    {
      "epoch": 244.23963133640552,
      "grad_norm": 0.23400413990020752,
      "learning_rate": 1.8596311475409836e-05,
      "loss": 0.1823,
      "step": 159000
    },
    {
      "epoch": 245.00768049155147,
      "grad_norm": 0.24978169798851013,
      "learning_rate": 1.834016393442623e-05,
      "loss": 0.1821,
      "step": 159500
    },
    {
      "epoch": 245.77572964669739,
      "grad_norm": 0.3107442259788513,
      "learning_rate": 1.8084016393442625e-05,
      "loss": 0.1823,
      "step": 160000
    },
    {
      "epoch": 246.54377880184333,
      "grad_norm": 0.6074236631393433,
      "learning_rate": 1.7827868852459018e-05,
      "loss": 0.1821,
      "step": 160500
    },
    {
      "epoch": 247.31182795698925,
      "grad_norm": 0.4313226342201233,
      "learning_rate": 1.757172131147541e-05,
      "loss": 0.1821,
      "step": 161000
    },
    {
      "epoch": 248.07987711213516,
      "grad_norm": 0.23618163168430328,
      "learning_rate": 1.7315573770491804e-05,
      "loss": 0.1819,
      "step": 161500
    },
    {
      "epoch": 248.8479262672811,
      "grad_norm": 0.2556956112384796,
      "learning_rate": 1.7059426229508198e-05,
      "loss": 0.1821,
      "step": 162000
    },
    {
      "epoch": 249.61597542242703,
      "grad_norm": 0.511567234992981,
      "learning_rate": 1.680327868852459e-05,
      "loss": 0.1821,
      "step": 162500
    },
    {
      "epoch": 250.38402457757297,
      "grad_norm": 0.4008665382862091,
      "learning_rate": 1.6547131147540984e-05,
      "loss": 0.1817,
      "step": 163000
    },
    {
      "epoch": 251.1520737327189,
      "grad_norm": 0.39535045623779297,
      "learning_rate": 1.6290983606557377e-05,
      "loss": 0.182,
      "step": 163500
    },
    {
      "epoch": 251.92012288786484,
      "grad_norm": 0.3440856337547302,
      "learning_rate": 1.6034836065573773e-05,
      "loss": 0.182,
      "step": 164000
    },
    {
      "epoch": 252.68817204301075,
      "grad_norm": 0.3876744210720062,
      "learning_rate": 1.5778688524590166e-05,
      "loss": 0.1819,
      "step": 164500
    },
    {
      "epoch": 253.45622119815667,
      "grad_norm": 0.24118047952651978,
      "learning_rate": 1.552254098360656e-05,
      "loss": 0.1818,
      "step": 165000
    },
    {
      "epoch": 254.22427035330261,
      "grad_norm": 0.30175721645355225,
      "learning_rate": 1.526639344262295e-05,
      "loss": 0.1815,
      "step": 165500
    },
    {
      "epoch": 254.99231950844853,
      "grad_norm": 0.2577589154243469,
      "learning_rate": 1.5010245901639344e-05,
      "loss": 0.1819,
      "step": 166000
    },
    {
      "epoch": 255.76036866359448,
      "grad_norm": 0.3328475058078766,
      "learning_rate": 1.4754098360655739e-05,
      "loss": 0.1817,
      "step": 166500
    },
    {
      "epoch": 256.5284178187404,
      "grad_norm": 0.25505149364471436,
      "learning_rate": 1.4497950819672132e-05,
      "loss": 0.1814,
      "step": 167000
    },
    {
      "epoch": 257.2964669738863,
      "grad_norm": 0.19586984813213348,
      "learning_rate": 1.4241803278688525e-05,
      "loss": 0.182,
      "step": 167500
    },
    {
      "epoch": 258.06451612903226,
      "grad_norm": 0.3392810821533203,
      "learning_rate": 1.398565573770492e-05,
      "loss": 0.1814,
      "step": 168000
    },
    {
      "epoch": 258.8325652841782,
      "grad_norm": 0.21088925004005432,
      "learning_rate": 1.3729508196721313e-05,
      "loss": 0.1816,
      "step": 168500
    },
    {
      "epoch": 259.6006144393241,
      "grad_norm": 0.5910347700119019,
      "learning_rate": 1.3473360655737704e-05,
      "loss": 0.1814,
      "step": 169000
    },
    {
      "epoch": 260.36866359447004,
      "grad_norm": 0.4096745550632477,
      "learning_rate": 1.32172131147541e-05,
      "loss": 0.1816,
      "step": 169500
    },
    {
      "epoch": 261.136712749616,
      "grad_norm": 0.4717288315296173,
      "learning_rate": 1.2961065573770492e-05,
      "loss": 0.1815,
      "step": 170000
    },
    {
      "epoch": 261.9047619047619,
      "grad_norm": 0.3812073767185211,
      "learning_rate": 1.2704918032786885e-05,
      "loss": 0.1815,
      "step": 170500
    },
    {
      "epoch": 262.6728110599078,
      "grad_norm": 0.43374618887901306,
      "learning_rate": 1.244877049180328e-05,
      "loss": 0.1813,
      "step": 171000
    },
    {
      "epoch": 263.44086021505376,
      "grad_norm": 0.3678436279296875,
      "learning_rate": 1.2192622950819672e-05,
      "loss": 0.1816,
      "step": 171500
    },
    {
      "epoch": 264.2089093701997,
      "grad_norm": 0.36662614345550537,
      "learning_rate": 1.1936475409836066e-05,
      "loss": 0.181,
      "step": 172000
    },
    {
      "epoch": 264.9769585253456,
      "grad_norm": 0.37008166313171387,
      "learning_rate": 1.168032786885246e-05,
      "loss": 0.1815,
      "step": 172500
    },
    {
      "epoch": 265.74500768049154,
      "grad_norm": 0.1680728942155838,
      "learning_rate": 1.1424180327868853e-05,
      "loss": 0.1811,
      "step": 173000
    },
    {
      "epoch": 266.5130568356375,
      "grad_norm": 0.18500077724456787,
      "learning_rate": 1.1168032786885246e-05,
      "loss": 0.1815,
      "step": 173500
    },
    {
      "epoch": 267.28110599078343,
      "grad_norm": 0.6891267895698547,
      "learning_rate": 1.091188524590164e-05,
      "loss": 0.1812,
      "step": 174000
    },
    {
      "epoch": 268.0491551459293,
      "grad_norm": 0.20037320256233215,
      "learning_rate": 1.0655737704918032e-05,
      "loss": 0.1812,
      "step": 174500
    },
    {
      "epoch": 268.81720430107526,
      "grad_norm": 0.4082583785057068,
      "learning_rate": 1.0399590163934427e-05,
      "loss": 0.1812,
      "step": 175000
    },
    {
      "epoch": 269.5852534562212,
      "grad_norm": 0.3286798894405365,
      "learning_rate": 1.014344262295082e-05,
      "loss": 0.1812,
      "step": 175500
    },
    {
      "epoch": 270.35330261136716,
      "grad_norm": 0.3513008952140808,
      "learning_rate": 9.887295081967215e-06,
      "loss": 0.1811,
      "step": 176000
    },
    {
      "epoch": 271.12135176651304,
      "grad_norm": 0.4796084761619568,
      "learning_rate": 9.631147540983606e-06,
      "loss": 0.1811,
      "step": 176500
    },
    {
      "epoch": 271.889400921659,
      "grad_norm": 0.6861513257026672,
      "learning_rate": 9.375000000000001e-06,
      "loss": 0.1813,
      "step": 177000
    },
    {
      "epoch": 272.65745007680493,
      "grad_norm": 0.23829220235347748,
      "learning_rate": 9.118852459016394e-06,
      "loss": 0.1809,
      "step": 177500
    },
    {
      "epoch": 273.4254992319508,
      "grad_norm": 0.2173014134168625,
      "learning_rate": 8.862704918032787e-06,
      "loss": 0.181,
      "step": 178000
    },
    {
      "epoch": 274.19354838709677,
      "grad_norm": 0.747671902179718,
      "learning_rate": 8.60655737704918e-06,
      "loss": 0.1811,
      "step": 178500
    },
    {
      "epoch": 274.9615975422427,
      "grad_norm": 0.4156491756439209,
      "learning_rate": 8.350409836065575e-06,
      "loss": 0.181,
      "step": 179000
    },
    {
      "epoch": 275.72964669738866,
      "grad_norm": 0.2634170651435852,
      "learning_rate": 8.094262295081968e-06,
      "loss": 0.1812,
      "step": 179500
    },
    {
      "epoch": 276.49769585253455,
      "grad_norm": 0.18335948884487152,
      "learning_rate": 7.838114754098361e-06,
      "loss": 0.181,
      "step": 180000
    },
    {
      "epoch": 277.2657450076805,
      "grad_norm": 0.2420584112405777,
      "learning_rate": 7.581967213114754e-06,
      "loss": 0.1807,
      "step": 180500
    },
    {
      "epoch": 278.03379416282644,
      "grad_norm": 0.3214731514453888,
      "learning_rate": 7.325819672131148e-06,
      "loss": 0.181,
      "step": 181000
    },
    {
      "epoch": 278.8018433179723,
      "grad_norm": 0.33516398072242737,
      "learning_rate": 7.0696721311475405e-06,
      "loss": 0.1808,
      "step": 181500
    },
    {
      "epoch": 279.5698924731183,
      "grad_norm": 0.48393380641937256,
      "learning_rate": 6.8135245901639345e-06,
      "loss": 0.1806,
      "step": 182000
    },
    {
      "epoch": 280.3379416282642,
      "grad_norm": 0.5664598345756531,
      "learning_rate": 6.557377049180328e-06,
      "loss": 0.1812,
      "step": 182500
    },
    {
      "epoch": 281.10599078341016,
      "grad_norm": 0.40723127126693726,
      "learning_rate": 6.301229508196721e-06,
      "loss": 0.1809,
      "step": 183000
    },
    {
      "epoch": 281.87403993855605,
      "grad_norm": 0.17264071106910706,
      "learning_rate": 6.045081967213115e-06,
      "loss": 0.181,
      "step": 183500
    },
    {
      "epoch": 282.642089093702,
      "grad_norm": 0.4590073823928833,
      "learning_rate": 5.7889344262295086e-06,
      "loss": 0.1809,
      "step": 184000
    },
    {
      "epoch": 283.41013824884794,
      "grad_norm": 0.48864516615867615,
      "learning_rate": 5.532786885245902e-06,
      "loss": 0.1807,
      "step": 184500
    },
    {
      "epoch": 284.17818740399383,
      "grad_norm": 0.3116641938686371,
      "learning_rate": 5.276639344262296e-06,
      "loss": 0.1806,
      "step": 185000
    },
    {
      "epoch": 284.9462365591398,
      "grad_norm": 0.4656924605369568,
      "learning_rate": 5.020491803278689e-06,
      "loss": 0.1808,
      "step": 185500
    },
    {
      "epoch": 285.7142857142857,
      "grad_norm": 0.3445911705493927,
      "learning_rate": 4.764344262295082e-06,
      "loss": 0.1808,
      "step": 186000
    },
    {
      "epoch": 286.48233486943167,
      "grad_norm": 0.42182356119155884,
      "learning_rate": 4.508196721311476e-06,
      "loss": 0.1809,
      "step": 186500
    },
    {
      "epoch": 287.25038402457756,
      "grad_norm": 0.38620439171791077,
      "learning_rate": 4.252049180327869e-06,
      "loss": 0.1805,
      "step": 187000
    },
    {
      "epoch": 288.0184331797235,
      "grad_norm": 0.5001453161239624,
      "learning_rate": 3.995901639344263e-06,
      "loss": 0.181,
      "step": 187500
    },
    {
      "epoch": 288.78648233486945,
      "grad_norm": 0.3157450258731842,
      "learning_rate": 3.739754098360656e-06,
      "loss": 0.1808,
      "step": 188000
    },
    {
      "epoch": 289.55453149001534,
      "grad_norm": 0.3225831985473633,
      "learning_rate": 3.483606557377049e-06,
      "loss": 0.1805,
      "step": 188500
    },
    {
      "epoch": 290.3225806451613,
      "grad_norm": 0.3939940631389618,
      "learning_rate": 3.227459016393443e-06,
      "loss": 0.1807,
      "step": 189000
    },
    {
      "epoch": 291.0906298003072,
      "grad_norm": 0.36138486862182617,
      "learning_rate": 2.971311475409836e-06,
      "loss": 0.1808,
      "step": 189500
    },
    {
      "epoch": 291.8586789554532,
      "grad_norm": 0.27514851093292236,
      "learning_rate": 2.7151639344262296e-06,
      "loss": 0.1808,
      "step": 190000
    },
    {
      "epoch": 292.62672811059906,
      "grad_norm": 0.23377300798892975,
      "learning_rate": 2.459016393442623e-06,
      "loss": 0.1805,
      "step": 190500
    },
    {
      "epoch": 293.394777265745,
      "grad_norm": 0.48723384737968445,
      "learning_rate": 2.2028688524590167e-06,
      "loss": 0.1806,
      "step": 191000
    },
    {
      "epoch": 294.16282642089095,
      "grad_norm": 0.31251856684684753,
      "learning_rate": 1.94672131147541e-06,
      "loss": 0.1809,
      "step": 191500
    },
    {
      "epoch": 294.93087557603684,
      "grad_norm": 0.5191187262535095,
      "learning_rate": 1.6905737704918035e-06,
      "loss": 0.1806,
      "step": 192000
    },
    {
      "epoch": 295.6989247311828,
      "grad_norm": 0.2649136483669281,
      "learning_rate": 1.4344262295081968e-06,
      "loss": 0.1805,
      "step": 192500
    },
    {
      "epoch": 296.46697388632873,
      "grad_norm": 0.2627700865268707,
      "learning_rate": 1.1782786885245902e-06,
      "loss": 0.1805,
      "step": 193000
    },
    {
      "epoch": 297.2350230414747,
      "grad_norm": 0.33716338872909546,
      "learning_rate": 9.221311475409837e-07,
      "loss": 0.1809,
      "step": 193500
    },
    {
      "epoch": 298.00307219662056,
      "grad_norm": 0.31123632192611694,
      "learning_rate": 6.65983606557377e-07,
      "loss": 0.1806,
      "step": 194000
    },
    {
      "epoch": 298.7711213517665,
      "grad_norm": 0.30476143956184387,
      "learning_rate": 4.098360655737705e-07,
      "loss": 0.1806,
      "step": 194500
    },
    {
      "epoch": 299.53917050691246,
      "grad_norm": 0.23509912192821503,
      "learning_rate": 1.5368852459016395e-07,
      "loss": 0.1806,
      "step": 195000
    }
  ],
  "logging_steps": 500,
  "max_steps": 195300,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 300,
  "save_steps": 1000,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": true
      },
      "attributes": {}
    }
  },
  "total_flos": 0.0,
  "train_batch_size": 1024,
  "trial_name": null,
  "trial_params": null
}
